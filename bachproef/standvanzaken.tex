\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.

Dit hoofdstuk bevat je literatuurstudie. De inhoud gaat verder op de inleiding, maar zal het onderwerp van de bachelorproef *diepgaand* uitspitten. De bedoeling is dat de lezer na lezing van dit hoofdstuk helemaal op de hoogte is van de huidige stand van zaken (state-of-the-art) in het onderzoeksdomein. Iemand die niet vertrouwd is met het onderwerp, weet nu voldoende om de rest van het verhaal te kunnen volgen, zonder dat die er nog andere informatie moet over opzoeken \autocite{Pollefliet2011}.

Je verwijst bij elke bewering die je doet, vakterm die je introduceert, enz.\ naar je bronnen. In \LaTeX{} kan dat met het commando \texttt{$\backslash${textcite\{\}}} of \texttt{$\backslash${autocite\{\}}}. Als argument van het commando geef je de ``sleutel'' van een ``record'' in een bibliografische databank in het Bib\LaTeX{}-formaat (een tekstbestand). Als je expliciet naar de auteur verwijst in de zin (narratieve referentie), gebruik je \texttt{$\backslash${}textcite\{\}}. Soms is de auteursnaam niet expliciet een onderdeel van de zin, dan gebruik je \texttt{$\backslash${}autocite\{\}} (referentie tussen haakjes). Dit gebruik je bv.~bij een citaat, of om in het bijschrift van een overgenomen afbeelding, broncode, tabel, enz. te verwijzen naar de bron. In de volgende paragraaf een voorbeeld van elk.

\textcite{Knuth1998} schreef een van de standaardwerken over sorteer- en zoekalgoritmen. Experten zijn het erover eens dat cloud computing een interessante opportuniteit vormen, zowel voor gebruikers als voor dienstverleners op vlak van informatietechnologie~\autocite{Creeger2009}.

Let er ook op: het \texttt{cite}-commando voor de punt, dus binnen de zin. Je verwijst meteen naar een bron in de eerste zin die erop gebaseerd is, dus niet pas op het einde van een paragraaf.

\section{Natural Language Processing: Een Overzicht en Toepassingen}

\subsection{Definitie en Belang van NLP}

Natural Language Processing (NLP) is een cruciale subdiscipline binnen de kunstmatige intelligentie die zich bezighoudt met de interactie tussen computers en menselijke (natuurlijke) talen. Het stelt machines in staat om menselijke taal te begrijpen en interpreteren, wat essentieel is voor het efficiënt verwerken van grote hoeveelheden tekstuele en spraakgegevens. De ontwikkeling van NLP heeft de manier waarop machines menselijke taal analyseren fundamenteel veranderd, waardoor ze niet alleen tekst kunnen begrijpen maar, ook reageren in een manier die voorheen voorbehouden was aan menselijke interactie \autocite{Sanadi2022}.

\subsection{Toepassingen van NLP}

De toepassingen van NLP zijn divers en invloedrijk in verschillende domeinen, van machine vertaling en sentimentanalyse tot stem gestuurde assistenten. Deze technologieën worden steeds vaker geïmplementeerd in apparaten en diensten die we dagelijks gebruiken, wat aantoont hoe diepgaand NLP de interactie tussen mens en machine heeft getransformeerd. Door de integratie van NLP in deze applicaties kunnen apparaten complexe menselijke commando's begrijpen en hierop reageren, wat de bruikbaarheid en toegankelijkheid van technologie verbetert \autocite{Feng2020}.

\subsection{Privacyvoordelen van Lokale NLP}

Een belangrijke ontwikkeling binnen NLP is de implementatie van deze technologieën direct op lokale apparaten, zoals Android smartphones. Door NLP lokaal uit te voeren, kunnen gebruikers profiteren van de voordelen die bij taalverwerking komen, zonder hun gegevens naar externe servers te sturen. Dit biedt aanzienlijke privacyvoordelen, aangezien gevoelige informatie op het apparaat zelf wordt verwerkt en beheerd, waardoor het risico op datalekken en externe toegang tot persoonlijke gegevens wordt verminderd. Deze benadering is vooral belangrijk in toepassingen waarbij gevoelige gegevens worden verwerkt, zoals in medische, financiële of persoonlijke assistent-applicaties \autocite{Locke2021Natural}.

\subsection{Conclusie}

De integratie van NLP in zowel dagelijkse als gespecialiseerde technologieën heeft aanzienlijke voordelen voor het begrijpen en verwerken van menselijke taal. Door deze systemen lokaal te implementeren, kunnen ontwikkelaars en eindgebruikers genieten van zowel functionele als privacy-voordelen, wat essentieel is in onze s\-t\-ee\-ds meer verbonden en gegevensgevoelige wereld.

\section{AI-Modellen voor Mobiele Apparaten: Een Focus op BERT, DistilBERT en TinyBERT}

\subsection{Overzicht van Modellen}

Recente ontwikkelingen in AI-modellen hebben geleid tot aanzienlijke vooruitgang in natuurlijke taalverwerking (NLP). Modellen zoals BERT (Bidirectional Encoder Representations from Transformers) en zijn afgeleiden, DistilBERT en TinyBERT, spelen een cruciale rol. BERT-modellen zijn met name effectief in het begrijpen van de context van een woord binnen een zin, wat hen zeer geschikt maakt voor complexe NLP-taken. Echter, vanwege hun grootte en complexiteit, zijn deze modellen vaak niet direct geschikt voor gebruik op mobiele apparaten met beperkte bronnen. Om dit probleem aan te pakken, zijn DistilBERT en TinyBERT ontwikkeld als kleinere, efficiëntere versies die beter geschikt zijn voor mobiele toepassingen. TinyBERT, bijvoorbeeld, is speciaal ontworpen voor kennisdestillatie van Transformer-gebaseerde modellen, wat resulteert in een model dat aanzienlijk kleiner en sneller is dan zijn voorganger, zonder aanzienlijke compromissen in de prestaties \autocite{Jiao2019TinyBERT}.

\subsection{Criteria voor Modelkeuze}

Bij het kiezen van een taalmodel voor mobiele apparaten zijn er enkele belangrijke criteria te overwegen, waaronder modelgrootte, nauwkeurigheid en de benodigde rekenkracht. Kleinere modellen zoals TinyBERT en DistilBERT zijn vaak de voorkeursopties omdat ze minder opslagruimte en verwerkingscapaciteit vereisen. Deze modellen behouden een aanzienlijke mate van nauwkeurigheid, terwijl ze aanzienlijk sneller en lichter zijn, wat essentieel is voor toepassingen op mobiele apparaten \autocite{Sun2020MobileBERT}.

\subsection{Vergelijking van Modellen}

TinyBERT en DistilBERT bieden beide aanzienlijke voordelen ten opzichte van het oorspronkelijke BERT-model wat betreft inzetbaarheid op mobiele apparaten. TinyBERT, bijvoorbeeld, biedt vergelijkbare prestaties als BERT-Base maar met slechts 28\% van de parameters en 31\% van de inferentietijd. DistilBERT daarentegen behoudt ongeveer 97\% van de taalbegripscapaciteit van BERT, terwijl het 40\% kleiner is en 60\% sneller dan het originele BERT-model. Deze modellen illustreren hoe kennisdestillatie kan worden gebruikt om krachtige, doch lichte modellen te creëren die geschikt zijn voor gebruik in resource-beperkte omgevingen zoals mobiele apparaten \autocite{Sanh2019DistilBERT}.


\section{BERT, RobBERT, en RobBERTje: Prestaties en Optimalisaties van Nederlandse Taalmodellen}

\subsection{Inleiding tot BERT}

BERT (Bidirectional Encoder Representations from Transformers) is een trans\-for\-mer-gebaseerd model dat heeft gezorgd voor aanzienlijke verbeteringen in verschillende natuurlijke taaltaken door zijn vermogen om diepe bidirectionele representaties te pre-trainen. Dit model kan fijn afgesteld worden met slechts één extra outputlaag om toonaangevende resultaten te leveren voor een breed scala aan taken, zoals vraag beantwoording en taal inferentie, zonder substantiële aanpassingen in de taakspecifieke architectuur \autocite{Devlin2019}.

\subsection{RobBERT en RobBERTje}

RobBERT en RobBERTje zijn Nederlandse varianten van het BERT-model, ontwikkeld om beter te presteren op taaltaken specifiek voor het Nederlands. RobBERT gebruikt een modelarchitectuur vergelijkbaar met die van BERT maar is getraind op een rijke dataset van Nederlandse teksten, wat resulteert in verbeterde prestaties op downstream NLP-taken zoals part-of-speech tagging en naamherkenning. RobBERTje is een verder gedistilleerde versie, gecreëerd met zicht op een efficiëntie in grote en snelheid voor mobiele toepassingen \autocite{Vries2019}.

\subsection{Vergelijking van Distillatiearchitecturen}

Distillatiearchitecturen zoals TinyBERT, DistilBERT en RobBERTje hebben elk unieke benaderingen en optimalisaties die hen onderscheiden, vooral in hoe ze presteren en hoe ze worden getraind.

\textbf{TinyBERT} maakt gebruik van een tweefasig leerproces dat zowel tijdens de pre-training als taakspecifieke afstemming plaatsvindt. Deze methode zorgt ervoor dat TinyBERT zowel algemene als taakspecifieke kennis efficiënt kan overnemen van zijn 'leraar'-model. TinyBERT slaagt erin om met slechts 28\% van de parameters en 31\% van de inferentietijd vergeleken met zijn leraar, BERT-Base, toch meer dan 96.8\% van de prestaties te behouden op de GLUE benchmark. Deze indrukwekkende reductie in grootte en snelheid maakt TinyBERT bijzonder geschikt voor mobiele en ingebedde toepassingen \autocite{Jiao2019TinyBERT}.

\textbf{DistilBERT}, aan de andere kant, is ontworpen om de grootte van het BERT-model met 40\% te verminderen, terwijl nog steeds 97\% van zijn taalbegripscapaciteiten behouden blijft. Dit wordt bereikt door kennisdestillatie toe te passen tijdens de pre-trainingsfase. DistilBERT gebruikt een 'triple loss' die taalmodellering, distillatie en cosinusafstandsverlies combineert, wat bijdraagt aan een efficiënte overdracht van kennis en een snellere inferentie, waardoor het een kosteneffectieve keuze is voor op apparaat gebaseerde berekeningen \autocite{Sanh2019DistilBERT}.

\textbf{RobBERTje} richt zich op het optimaliseren van de Nederlandse taalmodellen door verschillende distillatiecorpora te onderzoeken. Dit onderzoek heeft aangetoond dat het mengen van opeenvolgende zinnen in een corpus kan leiden tot modellen die sneller trainen en beter presteren op taken met lange sequenties. Dit suggereert dat de structuur van het trainingscorpus aanzienlijk kan bijdragen aan de effectiviteit van gedistilleerde modellen. Interessant is dat RobBERTje ook minder genderstereotype bias vertoont dan zijn leraarsmodel, wat wijst op een bijkomend voordeel van het distillatieproces \autocite{Delobelle2021}.

Deze diverse benaderingen van modeldistillatie illustreren de veelzijdigheid en aanpasbaarheid van distillatiearchitecturen in het verkleinen en versnellen van trans\-for\-mer-gebaseerde modellen, terwijl nog steeds een hoge mate van nauwkeurigheid behouden blijft. Elk van deze modellen biedt unieke voordelen, afhankelijk van de specifieke eisen en beperkingen van de toepassing, zoals geheugenbeperkingen, rekenkracht, en de specifieke kenmerken van de taak of het taaldomein.


\subsection{Bias en Efficiëntie}

Ondanks de efficiëntievoordelen van gedistilleerde modellen zoals RobBERTje, is het belangrijk om aandacht te besteden aan de bias die deze modellen kunnen bevatten. Het minimaliseren van bias terwijl de efficiëntie wordt gemaximaliseerd, blijft een belangrijk aandachtspunt voor de ontwikkeling van toekomstige NLP-modellen. Efficiëntie in training en inferentie is cruciaal voor de bruikbaarheid van deze modellen in real-world toepassingen, vooral op mobiele apparaten waar rekenkracht en geheugen beperkt zijn.


\subsection{Privacy en Gegevensbescherming}

- **Belang van Privacy**: Waarom lokale verwerking belangrijk is voor privacy en gegevensbescherming.
- **Gevoelige Data Verwerking**: Best practices voor het omgaan met gevoelige gegevens, inclusief data-encryptie en veilige communicatieprotocollen.
- **Privacy-overwegingen**: Overwegingen en maatregelen om de privacy van gebruikers te waarborgen.

\subsection{Huidige Oplossingen en Beperkingen}

- **Bestaande Methoden**: Analyse van bestaande toepassingen en methoden voor het integreren van AI op mobiele apparaten.
- **Voor- en Nadelen**: Voor- en nadelen van huidige oplossingen, inclusief technische en gebruiksaspecten.

\subsection{Model Conversie en Optimalisatie}

- **Model Conversie**: Stappen voor het converteren van modellen naar formaten die compatibel zijn met mobiele apparaten (bijv. TensorFlow Lite, ONNX).
- **Optimalisatietechnieken**: Beschrijving van optimalisatietechnieken zoals kwantisering, snoeien, en clustering om modelgrootte en inferentiesnelheid te verbeteren.

\subsection{Integratie van het Model in een Android Applicatie}

- **Mobiel-vriendelijke Deep Learning Frameworks**: Overzicht van frameworks zoals TensorFlow Lite en ONNX Runtime voor Android.
- **API's en Tools**: Beschikbare API's en tools voor modelimplementatie in Android-applicaties.
- **Preprocessing en Postprocessing**: Methoden voor data preprocessing, model inferentie, en output postprocessing.
- **Prestaties en Batterijoptimalisatie**: Technieken voor prestatie- en batterijoptimalisatie, zoals caching, achtergrondverwerking, en efficiënt geheugenbeheer.

\subsection{Soorten Android-diensten}

- **Android Input Method Editor (IME)**:
- **Voordelen en Nadelen**: Voor- en nadelen van het ontwikkelen van een IME voor tekstinvoer en taalmodelintegratie.
- **Implementatiestappen**: Stappen voor het opzetten en integreren van een IME.
- **Android System-level Service**:
- **Voordelen en Nadelen**: Voor- en nadelen van het ontwikkelen van een systeemdienst voor taalmodelgebaseerde functies.
- **Implementatiestappen**: Stappen voor het opzetten en integreren van een systeemdienst.

\subsection{Case Studies en Voorbeelden}

- **Relevante Voorbeelden**: Analyse van bestaande mobiele applicaties die AI-modellen lokaal draaien, inclusief succesverhalen en mislukkingen.
- **Lessons Learned**: Belangrijke lessen die getrokken kunnen worden uit bestaande case studies.
